# Multimodal-Transformer
Multimodal Emotion Recognition with Multiscale Feature Fusion with Inter-Intra modality Transformer (MMEMIT)
Introduction

Welcome to MMEMIT, a novel approach to emotion recognition that merges EEG data with facial images. Utilizing the DEAP dataset, this project aims to provide an insightful analysis of emotional states, combining neurological signals and facial expressions.

About MMEMIT

MMEMIT leverages advanced data processing techniques to integrate and analyze EEG signals and facial images. The project is primarily focused on harnessing the comprehensive DEAP dataset for emotion recognition, offering a unique perspective on the complex interplay between brain activity and facial cues in emotional expression.

Key Features
EEG Data Analysis: Harnesses EEG signals to explore the neurological basis of emotions.
Facial Image Processing: Uses advanced algorithms to interpret facial expressions related to emotional states.
Data Fusion: Combines EEG and facial image data for a multifaceted understanding of emotions.
Utilization of DEAP Dataset: Employs the DEAP dataset, a standard in emotion analysis research.
Getting Started

Prerequisites
Python 3.8 or higher
Libraries: NumPy, Pandas, TensorFlow, OpenCV
DEAP dataset
